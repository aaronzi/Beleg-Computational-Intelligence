{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kollisionsvermeidung - Modell-Training\n",
    "\n",
    "In diesem Notebook wird ein Bildklassifikator darauf trainiert die zwei Klassen\n",
    "``free`` und ``blocket`` zur Vermeidung von Kollisionen zu erkennen.  Dazu wird die Deep-Learning-Bibliothek *PyTorch* verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz hochladen und entpacken\n",
    "\n",
    "Bevor begonnen werden kann wird die ``dataset.zip`` Datei aus dem ``datensammlung.ipynb`` Notebook vom Roboter hochgeladen.\n",
    "\n",
    "Der folgende Befehl entpackt die Zip-Datei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sollte nun ein ``dataset`` Ordner erscheinen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz-Instanz erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wird die ``ImageFolder`` Datensatz-Klasse aus dem ``torchvision.datasets`` Paket genutzt. Weiterhin werden sogenannte transform aus dem ``torchvision.transforms`` Paket verwendet, um die Daten für das Trainieren vorzubereiten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    'dataset',\n",
    "    transforms.Compose([\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilen des Datensatzes in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wird der Datensatz in *training* und *test* Sets unterteilt. Die Test-Sets werden genutzt um die Genauigkeit/Performance des antrainierten Modells zu verifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 50, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders zum Laden der Daten in in Paketen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden zwei ``DataLoader`` Instanzen erzeugt, welche Fähigkeiten zum durchmischen von Daten bereitstellen, sowie *batches* von Bildern erzeugen, die dann parallel an das Modell übergeben werden können mittels meherere Worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definieren des neuronalen Netzes\n",
    "\n",
    "Nun wird das neuronale Netz deklariert, welches im Folgenden genutzt werden soll. Das *torchvision* Paket besitzt bereits eine sammlung von vortrainierten Modellen, die genutzt werden können.\n",
    "\n",
    "Über das *Transfer Learning* kann ein vortrainiertes Modell weiterverwertet werden für eine neue Aufgabe, für weelche deutlich weniger Daten zur Verfügung stehen.\n",
    "\n",
    "Wichtige Eigenschaften, die beim initialen Training des vorttrainierten Modells erlernt wurden, können für die neue Aufgabe weiterverwertet werden. Als neuronales Netzwerk wird anschließend das ``alexnet``-Modell genutzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ``alexnet``-Modell wurde ursprünglich mit einem Datensatz aus 1000 Klasen-Labeln trainiert, der nun genutze Datensatz besitz jedoch nur zwei Label. Folglich wird die letzte Layer mit einer neuen, untrainierten Layer ersetzt, welche nur zwei Ausgaben besitzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend wird das Modell auf die GPU verschoben, um dort berechnet zu werden\n",
    "\n",
    "> ``cuda`` sind die Rechenkerne einer Nvidia GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainieren das neuronalen Netzes\n",
    "\n",
    "Über den unter stehenden Code wird das neuronale Netz in 30 Epochen trainiert, wobei das beste Modell nach jeder Epoche gespeichert wird.\n",
    "\n",
    "> Eine Epoche ist ein Durchlauf durch den gesamten Trainingsdatensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BEST_MODEL_PATH = 'best_model.pth'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    test_error_count = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_error_count += float(torch.sum(torch.abs(labels - outputs.argmax(1))))\n",
    "    \n",
    "    test_accuracy = 1.0 - float(test_error_count) / float(len(test_dataset))\n",
    "    print('%d: %f' % (epoch, test_accuracy))\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wurde der Vorgang komplett abgeschlossen, sollte eine ``best_model.pth`` Datei im Jupyter Lab Dateibrowser erscheinen, welche anschließend heruntergaladen werden kann."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
